{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import scipy.misc\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from keras.applications import vgg19\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set backend as tensorflow\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering(\"tf\")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Start interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'block1_conv1', u'block1_conv2', u'block1_pool', u'block2_conv1', u'block2_conv2', u'block2_pool', u'block3_conv1', u'block3_conv2', u'block3_conv3', u'block3_conv4', u'block3_pool', u'block4_conv1', u'block4_conv2', u'block4_conv3', u'block4_conv4', u'block4_pool', u'block5_conv1', u'block5_conv2', u'block5_conv3', u'block5_conv4', u'block5_pool', u'input_2']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File(\"vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\", \"r\")\n",
    "print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "#     ('block1_conv1', 0.2),\n",
    "    ('block1_conv2', 5.0),\n",
    "#     ('block2_conv1', 0.2),\n",
    "    ('block2_conv2', 5.0),\n",
    "#     ('block3_conv1', 0.2),\n",
    "#     ('block3_conv2', 0.2),\n",
    "    ('block3_conv3', 5.0),\n",
    "#     ('block3_conv4', 0.2),\n",
    "#     ('block4_conv1', 0.2),\n",
    "#     ('block4_conv2', 0.2),\n",
    "#     ('block4_conv3', 0.2),\n",
    "    ('block4_conv4', 5.0),\n",
    "#     ('block5_conv1', 0.2),\n",
    "#     ('block5_conv2', 0.2),\n",
    "#     ('block5_conv3', 0.2),\n",
    "    ('block5_conv4', 5.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    IMAGE_WIDTH = 400\n",
    "    IMAGE_HEIGHT = 300\n",
    "    COLOR_CHANNELS = 3\n",
    "    CONTENT_WEIGHT = 0.025\n",
    "    STYLE_WEIGHT = 5.0\n",
    "    TOTAL_VARIATION_WEIGHT = 1.0\n",
    "    NOISE_RATIO = 0.6\n",
    "    MEANS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape_and_normalize_image(image_path):\n",
    "    \"\"\"\n",
    "    Reshape and normalize the input image (content or style)\n",
    "    \"\"\"\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((CONFIG.IMAGE_WIDTH, CONFIG.IMAGE_HEIGHT))\n",
    "    \n",
    "    image_array = np.asarray(image, dtype='float32')\n",
    "    image = np.expand_dims(image_array, axis=0)\n",
    "    \n",
    "    # Substract the mean to match the expected input of VGG16\n",
    "    image = image - CONFIG.MEANS\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_noise_image(content_image, noise_ratio = CONFIG.NOISE_RATIO):\n",
    "    \"\"\"\n",
    "    Generates a noisy image by adding random noise to the content_image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a random noise_image\n",
    "    noise_image = np.random.uniform(-20, 20, (1, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH,\n",
    "                                              CONFIG.COLOR_CHANNELS)).astype('float32')\n",
    "    \n",
    "    # Set the input_image to be a weighted average of the content_image and a noise_image\n",
    "    input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "    \n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_content_loss(content, generated):\n",
    "    m, n_H, n_W, n_C = generated.get_shape().as_list()\n",
    "\n",
    "    content = tf.reshape(content, [-1])\n",
    "    generated = tf.reshape(generated, [-1])\n",
    "\n",
    "    return tf.divide(tf.reduce_sum(tf.reduce_sum(tf.square( \\\n",
    "                    tf.subtract(content, generated)))), (4*n_H*n_W*n_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    gram = tf.matmul(x, x, transpose_b=True)\n",
    "    return gram\n",
    "\n",
    "def style_layer_loss(style, generated):\n",
    "    m, n_H, n_W, n_C = generated.get_shape().as_list()\n",
    "    \n",
    "    style = tf.reshape(tf.transpose(style), (n_C, -1))\n",
    "    generated = tf.reshape(tf.transpose(generated), (n_C, -1))\n",
    "    \n",
    "    S = gram_matrix(style)\n",
    "    G = gram_matrix(generated)\n",
    "\n",
    "    channels = n_C\n",
    "    size = n_H*n_W\n",
    "\n",
    "    return tf.divide(tf.reduce_sum(tf.reduce_sum(tf.square(tf.subtract(S, G)))),\n",
    "                     (4. * (channels ** 2) * (size ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get content_image from file\n",
    "content_image_path = \"./images/louvre_small.jpg\"\n",
    "content_image = reshape_and_normalize_image(content_image_path)\n",
    "\n",
    "image_to_show = plt.imread(content_image_path)\n",
    "# plt.imshow(image_to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_image_path = \"./images/monet.jpg\"\n",
    "style_image = reshape_and_normalize_image(style_image_path)\n",
    "\n",
    "image_to_show = plt.imread(content_image_path)\n",
    "# plt.imshow(image_to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a random noise_image\n",
    "generated_image = generate_noise_image(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combined input for model\n",
    "# input_tensor = K.concatenate([content_image,\n",
    "#                           style_image,\n",
    "#                           generated_image], axis=0)\n",
    "\n",
    "input_tensor = tf.Variable(np.zeros((1,\n",
    "                                     CONFIG.IMAGE_HEIGHT,\n",
    "                                     CONFIG.IMAGE_WIDTH,\n",
    "                                     CONFIG.COLOR_CHANNELS)),\n",
    "                           dtype='float32', name=\"input_tensor\")\n",
    "\n",
    "input_tensor = Input(tensor=input_tensor)\n",
    "model = vgg19.VGG19(weights=None,\n",
    "                    input_tensor=input_tensor,\n",
    "                    include_top=False)\n",
    "model.load_weights(\"vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "# outputs_list = [(layer_name, model.get_layer(layer_name).output)\n",
    "#                      for layer_name, _ in STYLE_LAYERS]\n",
    "\n",
    "# layer_outputs = [output_tensors for _, output_tensors in outputs_list]\n",
    "# print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# content_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "# content_model.load_weights(\"vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_input = model.get_layer(\"input_1\").output\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_loss(alpha=, beta=40):\n",
    "\n",
    "    # Assign the input of the model to be the \"content\" image \n",
    "    sess.run(input_tensor.assign(content_image))\n",
    "\n",
    "    # Get content loss from output of block 4, layer 2\n",
    "    out = model.get_layer(\"block4_conv2\").output\n",
    "    content_features = sess.run(out)\n",
    "    generated_features = tf.Variable(out)\n",
    "    content_loss = compute_content_loss(content_features, generated_features)\n",
    "    \n",
    "    # loss for style image\n",
    "    style_loss = 0.\n",
    "    \n",
    "    # Assign the input of the model to be the \"style\" image \n",
    "    sess.run(input_tensor.assign(style_image))\n",
    "    \n",
    "    for layer_name, coeff in STYLE_LAYERS:\n",
    "        # Select the output tensor of the currently selected layer\n",
    "        out = model.get_layer(layer_name).output\n",
    "        style_features = sess.run(out)\n",
    "        generated_features = tf.Variable(out)\n",
    "        style_loss += coeff * style_layer_loss(style_features, generated_features)\n",
    "    \n",
    "    # Get total loss using alpha and beta\n",
    "    total_loss = (content_loss*alpha) + (style_loss*beta)\n",
    "    \n",
    "    return content_loss, style_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_loss, style_loss, J = total_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5.0)\n",
    "\n",
    "# define train_step\n",
    "train_step = optimizer.minimize(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_image(path, image):\n",
    "    \n",
    "    # Un-normalize the image so that it looks good\n",
    "    image = image + CONFIG.MEANS\n",
    "    \n",
    "    # Clip and Save the image\n",
    "    image = np.clip(image[0], 0, 255).astype('uint8')\n",
    "    scipy.misc.imsave(path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_nn(sess, input_image, num_iterations = 200):\n",
    "    \n",
    "    # Initialize global variables (you need to run the session on the initializer)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Assign the input of the model to be the \"input\" image \n",
    "    sess.run(input_tensor.assign(input_image))\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Run the session on the train_step to minimize the total cost\n",
    "        sess.run([train_step])\n",
    "        \n",
    "        # Compute the generated image by running the session on the current model['input']\n",
    "        generated_image = sess.run(input_tensor)\n",
    "\n",
    "        # Print every 20 iteration.\n",
    "        if i%20 == 0:\n",
    "            Jt, Jc, Js = sess.run([J, content_loss, style_loss])\n",
    "            print(\"Iteration \" + str(i) + \" :\")\n",
    "            print(\"content cost = \", Jc)\n",
    "            print(\"style cost = \", Js)\n",
    "            print(\"total cost = \", Jt)\n",
    "            \n",
    "            # save current generated image in the \"/output\" directory\n",
    "            save_image(\"output/\" + str(i) + \".png\", generated_image)\n",
    "\n",
    "    # save last generated image\n",
    "    save_image('output/generated_image.jpg', generated_image)\n",
    "    \n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 :\n",
      "content cost =  109894.0\n",
      "style cost =  3.0094e+10\n",
      "total cost =  1.20376e+12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshbhardwaj/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20 :\n",
      "content cost =  96912.4\n",
      "style cost =  3.0094e+10\n",
      "total cost =  1.20376e+12\n",
      "Iteration 40 :\n",
      "content cost =  85800.5\n",
      "style cost =  3.0094e+10\n",
      "total cost =  1.20376e+12\n",
      "Iteration 60 :\n",
      "content cost =  76313.6\n",
      "style cost =  3.0094e+10\n",
      "total cost =  1.20376e+12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-aadd94edf756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(generated_image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-0082533c6df2>\u001b[0m in \u001b[0;36mmodel_nn\u001b[0;34m(sess, input_image, num_iterations)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Run the session on the train_step to minimize the total cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Compute the generated image by running the session on the current model['input']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/harshbhardwaj/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/harshbhardwaj/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/harshbhardwaj/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/harshbhardwaj/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/harshbhardwaj/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = model_nn(sess, generated_image)\n",
    "\n",
    "# print(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
